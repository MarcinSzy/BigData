{"paragraphs":[{"text":"\nval hc = new org.apache.spark.sql.hive.HiveContext(sc);","user":"anonymous","dateUpdated":"2019-02-06T17:57:45+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\nhc: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@1be324ea\n"}]},"apps":[],"jobName":"paragraph_1548531605416_-2039829734","id":"20190126-194005_1008241861","dateCreated":"2019-01-26T19:40:05+0000","dateStarted":"2019-01-31T20:45:52+0000","dateFinished":"2019-01-31T20:46:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:523"},{"text":"import org.apache.spark.sql.DataFrame\n\nval dfArray =  Array.ofDim[DataFrame](9,3);\n\ndfArray(0)(0) = hc.sql(\"select * FROM ODS.BADGESxml\");\ndfArray(0)(1) = hc.sql(\"select * FROM STG.BADGES\");\ndfArray(0)(2) = hc.sql(\"select * FROM SOR.BADGES\");\n\ndfArray(1)(0) = hc.sql(\"select * FROM ODS.COMMENTSxml\");\ndfArray(1)(1) = hc.sql(\"select * FROM STG.COMMENTS\");\ndfArray(1)(2) = hc.sql(\"select * FROM SOR.COMMENTS\");\n\ndfArray(2)(0) = hc.sql(\"select * FROM ODS.POSTSxml\");\ndfArray(2)(1) = hc.sql(\"select * FROM STG.POSTS\");\ndfArray(2)(2) = hc.sql(\"select * FROM SOR.POSTS\");\n\ndfArray(3)(0) = hc.sql(\"select * FROM ODS.POSTHISTORYxml\");\ndfArray(3)(1) = hc.sql(\"select * FROM STG.POSTHISTORY\");\ndfArray(3)(2) = hc.sql(\"select * FROM SOR.POSTHISTORY\");\n\ndfArray(4)(0) = hc.sql(\"select * FROM ODS.POSTLINKSxml\");\ndfArray(4)(1) = hc.sql(\"select * FROM STG.POSTLINKS\");\ndfArray(4)(2) = hc.sql(\"select * FROM SOR.POSTLINKS\");\n\ndfArray(5)(0) = hc.sql(\"select * FROM ODS.USERSxml\");\ndfArray(5)(1) = hc.sql(\"select * FROM STG.USERS\");\ndfArray(5)(2) = hc.sql(\"select * FROM SOR.USERS\");\n\ndfArray(6)(0) = hc.sql(\"select * FROM ODS.TAGSxml\");\ndfArray(6)(1) = hc.sql(\"select * FROM STG.TAGS\");\ndfArray(6)(2) = hc.sql(\"select * FROM SOR.TAGS\");\n\ndfArray(7)(0) = hc.sql(\"select * FROM ODS.VOTESxml\");\ndfArray(7)(1) = hc.sql(\"select * FROM STG.VOTES\");\ndfArray(7)(2) = hc.sql(\"select * FROM SOR.VOTES\");\n\n\n\n","user":"anonymous","dateUpdated":"2019-02-06T17:57:47+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.DataFrame\ndfArray: Array[Array[org.apache.spark.sql.DataFrame]] = Array(Array(null, null, null), Array(null, null, null), Array(null, null, null), Array(null, null, null), Array(null, null, null), Array(null, null, null), Array(null, null, null), Array(null, null, null), Array(null, null, null))\n"}]},"apps":[],"jobName":"paragraph_1548535386193_630352226","id":"20190126-204306_1982934000","dateCreated":"2019-01-26T20:43:06+0000","dateStarted":"2019-01-31T20:45:56+0000","dateFinished":"2019-01-31T20:47:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:524"},{"text":"var a = 0;\n    \n    //lista z nazwami tabel \n      val TableNames = List(\"BADGES\",\"COMMENTS\" ,\"POSTS\",\"POSTHISTORY\",\"POSTLINK\" ,\"USERS\", \"TAGS\", \"VOTES\");\n     \n     \n        var ODS_TAGS_CNT = sqlContext.emptyDataFrame;\n        var STG_TAGS_CNT = sqlContext.emptyDataFrame; \n        var SOR_TAGS_CNT = sqlContext.emptyDataFrame;\n        var LoadStatistic = sqlContext.emptyDataFrame;\n        var SingleTableLS = sqlContext.emptyDataFrame;\n        LoadStatistic = LoadStatistic.withColumn(\"TableNAme\",lit(\"\")).withColumn(\"TableNAme\",lit(\"\")).withColumn(\"ForumName\",lit(\"\")).withColumn(\"ODS_CNT\",lit(\"\")).withColumn(\"STG_CNT\",lit(\"\")).withColumn(\"SOR_CNT\",lit(\"\"));\n        \n        \n      //iteruj po wszystkich nazwach tabel i dla kazdej warsty dancyh wylicz liczebnosc zbiorow\n    for (a<-0 until TableNames.length ){\n        \n        // grupujac po nazwie forum wylicz ilosc rekordow w kazdym zbiorze \n        ODS_TAGS_CNT = dfArray(a)(0).groupBy(\"forumname\").count().withColumnRenamed(\"count\",\"ODS_CNT\").withColumn(\"SchemaName\", lit(\"ODS\")).withColumn(\"TableName\", lit(TableNames(a)));\n        STG_TAGS_CNT = dfArray(a)(1).groupBy(\"forumname\").count().withColumnRenamed(\"count\",\"STG_CNT\").withColumn(\"SchemaName\", lit(\"STG\")).withColumn(\"TableName\", lit(TableNames(a)));\n        SOR_TAGS_CNT = dfArray(a)(2).groupBy(\"forumname\").count().withColumnRenamed(\"count\",\"SOR_CNT\").withColumn(\"SchemaName\", lit(\"SOR\")).withColumn(\"TableName\", lit(TableNames(a)));\n        \n        //polacz wyszystkie zbiory \n       val SingleTableLS = ODS_TAGS_CNT.select(col(\"TableName\"),col(\"ODS_CNT\"),col(\"forumname\"))\n                            .join(STG_TAGS_CNT.select(col(\"TableName\"),col(\"STG_CNT\"),col(\"forumname\")),Seq(\"TableName\",\"forumname\"),\"left_outer\")\n                            .join(SOR_TAGS_CNT.select(col(\"TableName\"),col(\"SOR_CNT\"),col(\"forumname\")),Seq(\"TableName\",\"forumname\"),\"left_outer\")\n        \n        //po zakonczeniu kolejnej iteracji wykonaj union z tabela glowna \n        LoadStatistic=LoadStatistic.union(SingleTableLS);\n        \n      }\n\n    ","user":"anonymous","dateUpdated":"2019-02-06T17:57:47+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"a: Int = 0\nTableNames: List[String] = List(BADGES, COMMENTS, POSTS, POSTHISTORY, POSTLINK, USERS, TAGS, VOTES)\nODS_TAGS_CNT: org.apache.spark.sql.DataFrame = []\nSTG_TAGS_CNT: org.apache.spark.sql.DataFrame = []\nSOR_TAGS_CNT: org.apache.spark.sql.DataFrame = []\nLoadStatistic: org.apache.spark.sql.DataFrame = []\nSingleTableLS: org.apache.spark.sql.DataFrame = []\nLoadStatistic: org.apache.spark.sql.DataFrame = [TableNAme: string, ForumName: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1548538147842_1218645559","id":"20190126-212907_1745605348","dateCreated":"2019-01-26T21:29:07+0000","dateStarted":"2019-01-31T17:57:20+0000","dateFinished":"2019-01-31T17:57:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:525"},{"text":"val LoadStatisticFinal =LoadStatistic\n.withColumn(\"CheckDate\",lit(date_add (current_date,-1)))\n.withColumn(\"ODS_to_STG\", when(col(\"STG_CNT\").isNull, lit(0)).otherwise(col(\"STG_CNT\")) - when(col(\"ODS_CNT\").isNull, lit(0)).otherwise(col(\"ODS_CNT\")))\n.withColumn(\"STG_to_SOR\", when(col(\"SOR_CNT\").isNull, lit(0)).otherwise(col(\"SOR_CNT\")) - when(col(\"STG_CNT\").isNull, lit(0)).otherwise(col(\"STG_CNT\")))\n.withColumn(\"ODS_to_STG_flag\", when(col(\"ODS_to_STG\").notEqual(-3), lit(1)).otherwise(0))\n.withColumn(\"STG_to_SOR_flag\", when(col(\"STG_to_SOR\").notEqual(0), lit(1)).otherwise(0))\n\n","user":"anonymous","dateUpdated":"2019-02-06T17:57:48+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"LoadStatistic2: org.apache.spark.sql.DataFrame = [TableNAme: string, ForumName: string ... 8 more fields]\n"}]},"apps":[],"jobName":"paragraph_1548538920054_2051914798","id":"20190126-214200_1793506288","dateCreated":"2019-01-26T21:42:00+0000","dateStarted":"2019-01-31T17:59:38+0000","dateFinished":"2019-01-31T17:59:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:526"},{"text":"LoadStatistic2.filter(col(\"forumname\").equalTo(\"anime\") || col(\"forumname\").equalTo(\"arabic\") ).show();","user":"anonymous","dateUpdated":"2019-02-06T17:57:48+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------+---------+-------+-------+-------+----------+----------+----------+---------------+---------------+\n|  TableNAme|ForumName|ODS_CNT|STG_CNT|SOR_CNT| CheckDate|ODS_to_STG|STG_to_SOR|ODS_to_STG_flag|STG_to_SOR_flag|\n+-----------+---------+-------+-------+-------+----------+----------+----------+---------------+---------------+\n|     BADGES|   arabic|    923|    920|    920|2019-01-30|      -3.0|       0.0|              0|              0|\n|     BADGES|    anime|  44450|  44447|  44447|2019-01-30|      -3.0|       0.0|              0|              0|\n|   COMMENTS|    anime|  31174|   null|  31171|2019-01-30|  -31174.0|   31171.0|              1|              1|\n|   COMMENTS|   arabic|    532|   null|    529|2019-01-30|    -532.0|     529.0|              1|              1|\n|      POSTS|   arabic|    351|    348|    348|2019-01-30|      -3.0|       0.0|              0|              0|\n|      POSTS|    anime|  26733|  26730|  26730|2019-01-30|      -3.0|       0.0|              0|              0|\n|POSTHISTORY|   arabic|   1054|   1051|   1051|2019-01-30|      -3.0|       0.0|              0|              0|\n|POSTHISTORY|    anime|  97629|  97626|  97626|2019-01-30|      -3.0|       0.0|              0|              0|\n|   POSTLINK|    anime|   2875|   2872|   2872|2019-01-30|      -3.0|       0.0|              0|              0|\n|   POSTLINK|   arabic|     14|     11|     11|2019-01-30|      -3.0|       0.0|              0|              0|\n|      USERS|   arabic|    496|    493|   null|2019-01-30|      -3.0|    -493.0|              0|              1|\n|      USERS|    anime|  24626|  24623|   null|2019-01-30|      -3.0|  -24623.0|              0|              1|\n|       TAGS|    anime|   1415|   1412|   1412|2019-01-30|      -3.0|       0.0|              0|              0|\n|       TAGS|   arabic|     73|     70|     70|2019-01-30|      -3.0|       0.0|              0|              0|\n|      VOTES|    anime| 179990| 179987| 179987|2019-01-30|      -3.0|       0.0|              0|              0|\n|      VOTES|   arabic|   1474|   1471|   1471|2019-01-30|      -3.0|       0.0|              0|              0|\n+-----------+---------+-------+-------+-------+----------+----------+----------+---------------+---------------+\n\n"}]},"apps":[],"jobName":"paragraph_1548957590780_578937734","id":"20190131-175950_320063409","dateCreated":"2019-01-31T17:59:50+0000","dateStarted":"2019-01-31T18:07:12+0000","dateFinished":"2019-01-31T18:08:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:527"},{"text":"\nLoadStatistic.createOrReplaceTempView(\"LoadStatisticTemp\");\nsqlContext.sql(\"insert into table LoadStatistic  select * from LoadStatisticTemp\");\n","user":"anonymous","dateUpdated":"2019-02-06T17:57:49+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res653: org.apache.spark.sql.DataFrame = []\n"}]},"apps":[],"jobName":"paragraph_1548540196526_-932759208","id":"20190126-220316_1132514696","dateCreated":"2019-01-26T22:03:16+0000","dateStarted":"2019-01-27T09:25:26+0000","dateFinished":"2019-01-27T09:30:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:528"},{"text":"","user":"anonymous","dateUpdated":"2019-01-27T12:46:30+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"a: Array[String] = Array(id, userid, name, creationdate, class, tagbased, ppn_ev_id, forumname)\n"}]},"apps":[],"jobName":"paragraph_1548592438553_-36800713","id":"20190127-123358_977000633","dateCreated":"2019-01-27T12:33:58+0000","dateStarted":"2019-01-27T12:45:33+0000","dateFinished":"2019-01-27T12:45:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:529"},{"text":"\nimport util.control.Breaks._\n\nval TableNames = List(\"BADGES\",\"COMMENTS\" ,\"POSTS\",\"POSTHISTORY\",\"POSTLINK\" ,\"USERS\", \"TAGS\", \"VOTES\");\n\nval ColumnsNames = DataSet.columns;\nvar TableName=\"\";\nvar ColumnName=\"\"; \nvar Max=0;\nvar Min=0;\nvar Num_0=0;\nvar Num_null=0;\nvar avg=0;\nvar StatisticDF=sqlContext.emptyDataFrame;\n\n// tworzenie struktury dataframu  \nvar DataStatisticFinal = StatisticDF.withColumn(\"COLUMN_NAME\",lit(\"\")).withColumn(\"TABLE_NAME\",lit(\"\")).withColumn(\"forumname\",lit(\"\")).withColumn(\"MIN_VALUE\",lit(\"\")).withColumn(\"MAX_VALUE\",lit(\"\")).withColumn(\"DISTINCT_VALUE\",lit(\"\")).withColumn(\"CNT_VAL\",lit(\"\")).withColumn(\"NUMBER_OF_NULLS\",lit(\"\")).withColumn(\"ZEROS_OR_SPACE\",lit(\"\")).withColumn(\"AVG_VALUE\",lit(\"\"));\n\n// iteruj najpierw po tabelach a potem po każdej kolumnie w tabeli  \nfor (i<-0 until TableNames.length  ) {\n     println(\"TABLE_NAME:   \" + TableNames(i));\n     val DataSet = dfArray(i)(2)\n     val columnsDT = DataSet.schema.fields\n for (a<-0 until ColumnsNames.length){\n     \n     \n     var cdt =  columnsDT(a).dataType\n       println(cdt.toString())\n       \n       //w zaleznosci od typu danych w kolumnie nie wszystkie wartosci sa wyliczane \n        if (cdt.toString()==\"IntegerType\") {\n       \n        StatisticDF=DataSet.groupBy(col(\"forumname\")).agg( \n            min(ColumnsNames(a)).alias(\"MIN_VALUE\"), \n            max(ColumnsNames(a)).alias(\"MAX_VALUE\"),\n            countDistinct(ColumnsNames(a)).alias(\"DISTINCT_VALUE\"),\n            mean (ColumnsNames(a)).alias(\"AVG_VALUE\"), \n            count(ColumnsNames(a)).alias(\"CNT_VAL\"),\n            sum (  when( col(ColumnsNames(a)).equalTo(0) || col(ColumnsNames(a)).equalTo(\"\") , lit(1)).otherwise(0)).alias(\"ZEROS_OR_SPACE\"),\n            sum ( when( col(ColumnsNames(a)).isNull , lit(1)).otherwise(0)).alias(\"NUMBER_OF_NULLS\")\n    ).withColumn(\"TABLE_NAME\",lit(TableNames(i))).withColumn(\"COLUMN_NAME\",lit(ColumnsNames(a)))\n       \n        \n        StatisticDF = StatisticDF.select (\"COLUMN_NAME\" , \"TABLE_NAME\" , \"forumname\" , \"MIN_VALUE\" , \"MAX_VALUE\" , \"DISTINCT_VALUE\" , \"CNT_VAL\" , \"NUMBER_OF_NULLS\" , \"ZEROS_OR_SPACE\" , \"AVG_VALUE\" ) \n        \n        DataStatisticFinal = DataStatisticFinal.unionAll(StatisticDF)\n        \n       //val SingleTableLS = ODS_TAGS_CNT.select(col(\"TableName\"),col(\"ODS_CNT\"),col(\"forumname\")).join(STG_TAGS_CNT.select(col(\"TableName\"),col(\"STG_CNT\"),col(\"forumname\")),Seq(\"TableName\",\"forumname\"),\"left_outer\").join(SOR_TAGS_CNT.select(col(\"TableName\"),col(\"SOR_CNT\"),col//(\"forumname\")),Seq(\"TableName\",\"forumname\"),\"left_outer\")\n    } else if  (cdt.toString()==\"DateType\" || cdt.toString()==\"String\" ) \n    {\n           StatisticDF=DataSet.groupBy(col(\"forumname\")).agg( \n            min(ColumnsNames(a)).alias(\"MIN_VALUE\"), \n            max(ColumnsNames(a)).alias(\"MAX_VALUE\"),\n            countDistinct(ColumnsNames(a)).alias(\"DISTINCT_VALUE\"),\n            mean (lit(0)).alias(\"AVG_VALUE\"), \n            count(ColumnsNames(a)).alias(\"CNT_VAL\"),\n            sum ( lit(0)).alias(\"ZEROS_OR_SPACE\"),\n            sum ( when( col(ColumnsNames(a)).isNull , lit(1)).otherwise(0)).alias(\"NUMBER_OF_NULLS\")\n    ).withColumn(\"TABLE_NAME\",lit(TableNames(i))).withColumn(\"COLUMN_NAME\",lit(ColumnsNames(a)))\n       \n        \n        StatisticDF = StatisticDF.select (\"COLUMN_NAME\" , \"TABLE_NAME\" , \"forumname\" , \"MIN_VALUE\" , \"MAX_VALUE\" , \"DISTINCT_VALUE\" , \"CNT_VAL\" , \"NUMBER_OF_NULLS\" ,\"ZEROS_OR_SPACE\" , \"AVG_VALUE\" ) \n        \n        DataStatisticFinal = DataStatisticFinal.unionAll(StatisticDF)\n        \n    }\n    else if  ( cdt.toString()==\"String\" ) \n    {\n           StatisticDF=DataSet.groupBy(col(\"forumname\")).agg( \n            min(ColumnsNames(a)).alias(\"MIN_VALUE\"), \n            max(ColumnsNames(a)).alias(\"MAX_VALUE\"),\n            countDistinct(ColumnsNames(a)).alias(\"DISTINCT_VALUE\"),\n            mean (lit(0)).alias(\"AVG_VALUE\"), \n            count(ColumnsNames(a)).alias(\"CNT_VAL\"),\n            sum ( lit(0)).alias(\"ZEROS_OR_SPACE\"),\n            sum ( when( col(ColumnsNames(a)).isNull , lit(1)).otherwise(0)).alias(\"NUMBER_OF_NULLS\")\n    ).withColumn(\"TABLE_NAME\",lit(TableNames(i))).withColumn(\"COLUMN_NAME\",lit(ColumnsNames(a)))\n       \n        \n        StatisticDF = StatisticDF.select (\"COLUMN_NAME\" , \"TABLE_NAME\" , \"forumname\" , \"MIN_VALUE\" , \"MAX_VALUE\" , \"DISTINCT_VALUE\" , \"CNT_VAL\" , \"NUMBER_OF_NULLS\" ,\"ZEROS_OR_SPACE\" , \"AVG_VALUE\" ) \n        \n        DataStatisticFinal = DataStatisticFinal.unionAll(StatisticDF)\n        \n    }\n        \n    }\n    \n    \n}\n\n\n","user":"anonymous","dateUpdated":"2019-01-31T21:37:07+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"import util.control.Breaks._\nTableNames: List[String] = List(BADGES, COMMENTS, POSTS, POSTHISTORY, POSTLINK, USERS, TAGS, VOTES)\nColumnsNames: Array[String] = Array(id, userid, name, creationdate, creationts, class, tagbased, ppn_ev_id, forumname)\nTableName: String = \"\"\nColumnName: String = \"\"\nMax: Int = 0\nMin: Int = 0\nNum_0: Int = 0\nNum_null: Int = 0\navg: Int = 0\nStatisticDF: org.apache.spark.sql.DataFrame = []\nDataStatisticFinal: org.apache.spark.sql.DataFrame = [COLUMN_NAME: string, TABLE_NAME: string ... 8 more fields]\nwarning: there were three deprecation warnings; re-run with -deprecation for details\nTABLE_NAME:   BADGES\nIntegerType\nIntegerType\nStringType\nDateType\nTimestampType\nIntegerType\nBooleanType\nIntegerType\nStringType\nTABLE_NAME:   COMMENTS\nIntegerType\nIntegerType\nIntegerType\norg.apache.spark.sql.AnalysisException: cannot resolve '`name`' given input columns: [text, creationts, ppn_ev_id, creationdate, postid, userid, forumname, id, score];;\n'Aggregate [forumname#80], [forumname#80, min('name) AS MIN_VALUE#10994, max('name) AS MAX_VALUE#10996, count(distinct 'name) AS DISTINCT_VALUE#10998, avg('name) AS AVG_VALUE#11000, count('name) AS CNT_VAL#11003, sum(CASE WHEN (('name = 0) || ('name = )) THEN 1 ELSE 0 END) AS ZEROS_OR_SPACE#11005, sum(CASE WHEN isnull('name) THEN 1 ELSE 0 END) AS NUMBER_OF_NULLS#11007]\n+- Project [id#72, postid#73, score#74, text#75, creationdate#76, creationts#77, userid#78, ppn_ev_id#79, forumname#80]\n   +- SubqueryAlias comments\n      +- Relation[id#72,postid#73,score#74,text#75,creationdate#76,creationts#77,userid#78,ppn_ev_id#79,forumname#80] parquet\n\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)\n  at org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:64)\n  at org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:224)\n  at $$$$c3139809e277a348a4e7b84338995f$$$$fun$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(<console>:105)\n  at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n  at $$$$c550adb942e23da546dd2ad2a59b37b$$$$$anonfun$1.apply$mcVI$sp(<console>:96)\n  at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n  ... 76 elided\n"}]},"apps":[],"jobName":"paragraph_1548592509946_-26104365","id":"20190127-123509_2040129079","dateCreated":"2019-01-27T12:35:09+0000","dateStarted":"2019-01-31T21:37:07+0000","dateFinished":"2019-01-31T21:37:19+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:530"},{"text":"val DTX1 = DataSet.schema.fields(1).dataType\nval DTX2 = DataSet.schema.fields(1).dataType\n\nprintln (DTX1.toString()) ","user":"anonymous","dateUpdated":"2019-01-28T00:05:05+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"DTX1: org.apache.spark.sql.types.DataType = IntegerType\nDTX2: org.apache.spark.sql.types.DataType = IntegerType\nIntegerType\n"}]},"apps":[],"jobName":"paragraph_1548590829461_-1936208034","id":"20190127-120709_807786053","dateCreated":"2019-01-27T12:07:09+0000","dateStarted":"2019-01-28T00:05:06+0000","dateFinished":"2019-01-28T00:05:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:531"},{"text":" \nval cn = ColumnsNames(0)\nval TableNames = List(\"BADGES\",\"COMMENTS\" ,\"POSTS\",\"POSTHISTORY\",\"POSTLINK\" ,\"USERS\", \"TAGS\", \"VOTES\"); \n \n\n        StatisticDF=DataSet.groupBy(col(ColumnName)).agg(\n            min(\"id\").alias(\"MIN_VALUE\"),\n            max(\"id\").alias(\"MAX_VALUE\")\n           // countDistinct(\"id\").alias(\"DISTINCT_VALUE\"),\n           // mean (\"id\").alias(\"AVG_VALUE\"), \n           // count(\"id\").alias(\"CNT_VAL\"),\n           // sum (  when( col(\"id\").equalTo(0) || col(\"id\").equalTo(\"\") , lit(1)).otherwise(0)).alias(\"ZEROS_OR_SPACE\"),\n           // sum ( when( col(\"id\").isNull , lit(1)).otherwise(0)).alias(\"NUMBER_OF_NULLS\")\n    ).withColumn(\"TABLE_NAME\",lit(TableNames(0))).withColumn(\"COLUMN_NAME\",lit(\"id\"))","user":"anonymous","dateUpdated":"2019-01-31T21:22:45+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"cn: String = id\nTableNames: List[String] = List(BADGES, COMMENTS, POSTS, POSTHISTORY, POSTLINK, USERS, TAGS, VOTES)\norg.apache.spark.sql.AnalysisException: cannot resolve '``' given input columns: [userid, creationts, id, forumname, creationdate, class, tagbased, name, ppn_ev_id];;\n'Aggregate ['], [', min(id#22) AS MIN_VALUE#1633, max(id#22) AS MAX_VALUE#1635]\n+- Project [id#22, userid#23, name#24, creationdate#25, creationts#26, class#27, tagbased#28, ppn_ev_id#29, forumname#30]\n   +- SubqueryAlias badges\n      +- Relation[id#22,userid#23,name#24,creationdate#25,creationts#26,class#27,tagbased#28,ppn_ev_id#29,forumname#30] parquet\n\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:293)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:293)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)\n  at org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:64)\n  at org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:224)\n  ... 68 elided\n"}]},"apps":[],"jobName":"paragraph_1548964993187_-36515561","id":"20190131-200313_910559005","dateCreated":"2019-01-31T20:03:13+0000","dateStarted":"2019-01-31T21:22:45+0000","dateFinished":"2019-01-31T21:22:47+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:532"},{"text":"DataStatisticFinal.show()","user":"anonymous","dateUpdated":"2019-01-31T21:37:30+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------+----------+---------+----------+----------+--------------+-------+---------------+--------------+------------------+\n| COLUMN_NAME|TABLE_NAME|forumname| MIN_VALUE| MAX_VALUE|DISTINCT_VALUE|CNT_VAL|NUMBER_OF_NULLS|ZEROS_OR_SPACE|         AVG_VALUE|\n+------------+----------+---------+----------+----------+--------------+-------+---------------+--------------+------------------+\n|          id|    BADGES|    anime|         1|     58205|         44447|  44447|              0|             0| 30004.74250230612|\n|          id|    BADGES|   arabic|         1|       921|           920|    920|              0|             0| 460.8608695652174|\n|      userid|    BADGES|    anime|         1|     43700|         12928|  44447|              0|             0|14477.544333700813|\n|      userid|    BADGES|   arabic|         1|       498|           346|    920|              0|             0|169.79347826086956|\n|creationdate|    BADGES|    anime|2012-12-11|2018-12-02|          2181|  44447|              0|             0|               0.0|\n|creationdate|    BADGES|   arabic|2015-07-14|2015-09-14|            60|    920|              0|             0|               0.0|\n|       class|    BADGES|    anime|         1|         3|             3|  44447|              0|             0|2.8069161023241165|\n|       class|    BADGES|   arabic|         2|         3|             2|    920|              0|             0|2.9771739130434782|\n|   ppn_ev_id|    BADGES|    anime|         1|         1|             1|  44447|              0|             0|               1.0|\n|   ppn_ev_id|    BADGES|   arabic|         1|         1|             1|    920|              0|             0|               1.0|\n|          id|  COMMENTS|    anime|         2|     63918|         31171|  31171|              0|             0| 32452.95636970261|\n|          id|  COMMENTS|   arabic|         1|       613|           529|    529|              0|             0| 317.3421550094518|\n|      userid|  COMMENTS|    anime|         4|     43680|          2688|  30621|            550|             0| 9087.737990268117|\n|      userid|  COMMENTS|   arabic|         2|       494|            75|    526|              3|             0| 95.53041825095058|\n+------------+----------+---------+----------+----------+--------------+-------+---------------+--------------+------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1548959501123_1146758115","id":"20190131-183141_498975752","dateCreated":"2019-01-31T18:31:41+0000","dateStarted":"2019-01-31T21:37:31+0000","dateFinished":"2019-01-31T21:38:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:533"},{"text":"val PostsDF = dfArray(2)(2)\r\n\r\n\r\n\r\nPostsDF.withColumn(\"word\", explode(split(col(\"body\"), \" \")))\r\n    .groupBy(\"word\") \r\n    .count()\r\n    .sort(col(\"count\").desc)\r\n    .show(100)\r\n  ","user":"anonymous","dateUpdated":"2019-01-31T21:56:20+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"PostsDF: org.apache.spark.sql.DataFrame = [id: int, posttypeid: tinyint ... 21 more fields]\n+-------------+------+\n|         word| count|\n+-------------+------+\n|          the|175760|\n|           to| 84331|\n|           of| 76395|\n|          and| 66511|\n|            a| 65270|\n|           is| 59756|\n|           in| 52029|\n|         that| 45152|\n|             | 39572|\n|           it| 28236|\n|          was| 25917|\n|           as| 22736|\n|           he| 22077|\n|          for| 21467|\n|           be| 20044|\n|            I| 19770|\n|         with| 19549|\n|          are| 19262|\n|           on| 18172|\n|          his| 17902|\n|         this| 16404|\n|           <a| 16079|\n|          not| 15868|\n|         from| 15753|\n|         have| 15352|\n|          but| 15159|\n|           by| 15124|\n|           or| 14639|\n|         they| 14262|\n|rel=\"nofollow| 13015|\n|          can| 11562|\n|          you| 10423|\n|           at| 10404|\n|           an| 10288|\n|          her| 10175|\n|        would|  9990|\n|          has|  9851|\n|        which|  9599|\n|          she|  8911|\n|         when|  8817|\n|          The|  8648|\n|        their|  8577|\n|        there|  8564|\n|           so|  8509|\n|         like|  8412|\n|           if|  8398|\n|          one|  8275|\n|        anime|  8059|\n|        about|  7895|\n|         also|  7603|\n|         only|  7149|\n|          all|  7136|\n|           no|  7091|\n|         just|  7091|\n|          who|  6970|\n|        other|  6956|\n|         more|  6734|\n|         some|  6659|\n|         what|  6634|\n|      because|  6335|\n|         were|  5942|\n|          had|  5866|\n|          him|  5612|\n|           we|  5525|\n|        manga|  5474|\n|        could|  5419|\n|         into|  5374|\n|         will|  5309|\n|        after|  5303|\n|        first|  5210|\n|          any|  5166|\n|         been|  5119|\n|         know|  5077|\n|            -|  5049|\n|         it's|  4923|\n|          how|  4919|\n|        being|  4814|\n|           up|  4687|\n|         even|  4659|\n|           do|  4642|\n|         than|  4626|\n|         them|  4527|\n|          out|  4524|\n|      episode|  4506|\n|        where|  4345|\n|          see|  4290|\n|         time|  4237|\n|        image|  4180|\n|         same|  4176|\n|       series|  4122|\n|          use|  4072|\n|        think|  3964|\n|        don't|  3951|\n|         used|  3847|\n|           It|  3777|\n|          did|  3777|\n|         does|  3689|\n|  description|  3680|\n|          why|  3664|\n|         then|  3622|\n+-------------+------+\nonly showing top 100 rows\n\n"}]},"apps":[],"jobName":"paragraph_1548633123724_1086762585","id":"20190127-235203_1575994224","dateCreated":"2019-01-27T23:52:03+0000","dateStarted":"2019-01-31T21:56:21+0000","dateFinished":"2019-01-31T21:56:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:534"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1548970747805_-1903493489","id":"20190131-213907_658348265","dateCreated":"2019-01-31T21:39:07+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:535"}],"name":"SanityCheckeonData","id":"2E29JWNHY","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}